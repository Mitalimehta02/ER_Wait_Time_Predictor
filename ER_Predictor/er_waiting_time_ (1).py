# -*- coding: utf-8 -*-
"""ER waiting time .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14DAgniRSKksn61Zi66M_Vc3DwgraTd8W
"""

# GENERATING SYNTHETIC DATASET

import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta

# Set seed for reproducibility
np.random.seed(42)

# Generate random timestamps
def random_timestamp(start, end, n):
    return [start + timedelta(minutes=random.randint(0, int((end - start).total_seconds() / 60)))
            for _ in range(n)]

# Parameters
n_rows = 1000
start_time = datetime(2025, 6, 1, 0, 0)
end_time = datetime(2025, 6, 30, 23, 59)

timestamps = random_timestamp(start_time, end_time, n_rows)

# Sample symptom pool
symptom_pool = [
    "Patient reports chest pain and shortness of breath",
    "Mild headache and nausea",
    "Severe abdominal pain",
    "High fever and chills",
    "Minor cuts and bruises",
    "Difficulty breathing",
    "Broken ankle after fall",
    "Blurry vision and dizziness",
    "Vomiting and dehydration",
    "Skin rash and itching"
]

triage_levels = ["Low", "Medium", "High"]

# Build the dataset
data = {
    "arrival_time": timestamps,
    "queue_length": np.random.randint(0, 20, n_rows),
    "staff_available": np.random.randint(1, 10, n_rows),
    "symptom_text": np.random.choice(symptom_pool, n_rows)
}

df = pd.DataFrame(data)
df["day_of_week"] = df["arrival_time"].dt.day_name()
df["hour"] = df["arrival_time"].dt.hour

# Assign triage level probabilistically based on symptom_text
def assign_triage(symptom):
    if "chest pain" in symptom or "difficulty breathing" in symptom:
        return "High"
    elif "fever" in symptom or "abdominal pain" in symptom:
        return "Medium"
    else:
        return "Low"

df["triage_level"] = df["symptom_text"].apply(assign_triage)

# Wait time logic: higher queue + fewer staff + higher triage = longer wait
wait_base = 15
df["wait_time_minutes"] = (
    wait_base +
    df["queue_length"] * np.random.uniform(1.5, 2.5, n_rows) -
    df["staff_available"] * np.random.uniform(2.0, 3.0, n_rows) +
    df["triage_level"].map({"Low": 5, "Medium": 10, "High": 20})
).astype(int).clip(lower=0)

# Preview and save
df.head()
df.to_csv("synthetic_er_wait_data.csv", index=False)
print("Dataset saved as 'synthetic_er_wait_data.csv'")

# PHASE-1
# Step 1: Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Step 2: Load CSV
df = pd.read_csv("synthetic_er_wait_data.csv")
df.head()

# Step 3: Preprocessing

# Encode categorical features
le_day = LabelEncoder()
le_triage = LabelEncoder()

df['day_of_week_enc'] = le_day.fit_transform(df['day_of_week'])
df['triage_level_enc'] = le_triage.fit_transform(df['triage_level'])

# Select features and target
features = ['queue_length', 'staff_available', 'hour', 'day_of_week_enc', 'triage_level_enc']
target = 'wait_time_minutes'

X = df[features]
y = df[target]

# Step 4: Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Train Baseline Model (Random Forest)
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Step 6: Predictions and Evaluation
y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"ðŸ“Š MAE: {mae:.2f} minutes")
print(f"ðŸ“‰ RMSE: {rmse:.2f} minutes")
print(f"âœ… RÂ² Score: {r2:.2f}")

import matplotlib.pyplot as plt

importances = model.feature_importances_
plt.bar(features, importances)
plt.title("Feature Importance")
plt.xlabel("Feature")
plt.ylabel("Importance")
plt.show()

# PHASE-2

df = df.sort_values('arrival_time')  # sort chronologically

# Add rolling average of wait time based on the last 5 arrivals
df['rolling_avg_wait'] = df['wait_time_minutes'].rolling(window=5).mean().fillna(method='bfill')

# Update feature set
features = ['queue_length', 'staff_available', 'hour', 'day_of_week_enc', 'triage_level_enc', 'rolling_avg_wait']
X = df[features]
y = df['wait_time_minutes']

# Train/test split again
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training on XGBoost regressor
!pip install xgboost --quiet

import xgboost as xgb

xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)
xgb_model.fit(X_train, y_train)

y_pred_xgb = xgb_model.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_pred_xgb)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2 = r2_score(y_test, y_pred_xgb)

print(f"ðŸ“Š XGBoost MAE: {mae:.2f} mins")
print(f"ðŸ“‰ XGBoost RMSE: {rmse:.2f} mins")
print(f"âœ… XGBoost RÂ² Score: {r2:.2f}")

# PHASE-3
from sentence_transformers import SentenceTransformer

# Load pre-trained BERT sentence embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Encode the 'symptom_text' into 384-dim vectors
symptom_embeddings = model.encode(df['symptom_text'].tolist())

# Convert to DataFrame
import pandas as pd
embed_df = pd.DataFrame(symptom_embeddings, columns=[f"symptom_embed_{i}" for i in range(symptom_embeddings.shape[1])])

# Merge embeddings into main DataFrame
df_embed = pd.concat([df.reset_index(drop=True), embed_df.reset_index(drop=True)], axis=1)

# Final features: existing + LLM embeddings
final_features = features + list(embed_df.columns)

X = df_embed[final_features]
y = df_embed['wait_time_minutes']

# Train-test split again
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost with LLM
import xgboost as xgb

xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)
xgb_model.fit(X_train, y_train)

y_pred = xgb_model.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("ðŸ“Š With LLM-enhanced Features:")
print("MAE:", mean_absolute_error(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("RÂ² Score:", r2_score(y_test, y_pred))

from sklearn.model_selection import cross_val_score

# Compute cross-validated RÂ² scores (5-fold)
cv_scores = cross_val_score(xgb_model, X, y, scoring='r2', cv=5)
print(f"ðŸ“ˆ XGBoost 5-Fold CV RÂ² Scores: {cv_scores}")
print(f"ðŸ“Š XGBoost Average CV RÂ² Score: {cv_scores.mean():.3f}")

import matplotlib.pyplot as plt

# Scatter plot: Predicted vs Actual
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, alpha=0.5, edgecolor='k')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')  # identity line
plt.xlabel("Actual Wait Time (minutes)")
plt.ylabel("Predicted Wait Time (minutes)")
plt.title("ðŸ“Š Actual vs Predicted ER Wait Times")
plt.grid(True)
plt.tight_layout()
plt.show()

#SHAP (SHapley Additive exPlanations) is a game-theory-based method to explain how much each feature contributes to a specific prediction
!pip install shap --quiet
import shap

# Initialize TreeExplainer for your trained model
explainer = shap.Explainer(xgb_model)

# Compute SHAP values for a sample of your test set
shap_values = explainer(X_test.sample(100, random_state=42))

# Summary plot: which features are most important globally?
shap.plots.beeswarm(shap_values)

# explain a single Prediction
index = 10  # pick any index from X_test
shap.plots.waterfall(shap_values[index])

shap.plots.bar(shap_values)